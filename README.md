# Data Hunter Pro
### Agente AutÃ³nomo de Descubrimiento de Datos e Inteligencia Artificial

![Rust](https://img.shields.io/badge/Backend-Rust-black?style=flat&logo=rust)
![React](https://img.shields.io/badge/Frontend-React-blue?style=flat&logo=react)
![Docker](https://img.shields.io/badge/Deployment-Docker-2496ED?style=flat&logo=docker)
![OpenAI](https://img.shields.io/badge/AI-Powered-00A67E?style=flat&logo=openai)
![License](https://img.shields.io/badge/License-MIT-green)

**Data Hunter Pro** es una herramienta de alto rendimiento diseÃ±ada para rastrear, identificar y clasificar datasets de cÃ³digo abierto en la web. Combina la velocidad de **Rust** para el crawling web con la potencia de **OpenAI (GPT-4o-mini)** para analizar semÃ¡nticamente el contenido de los recursos encontrados.

Cuenta con una interfaz moderna en **React** que permite la monitorizaciÃ³n en tiempo real mediante **Server-Sent Events (SSE)**.

---

## âœ¨ CaracterÃ­sticas Principales

*   **ğŸš€ Motor de Alto Rendimiento:** Backend escrito en Rust utilizando `Tokio` y `Axum` para manejo asÃ­ncrono y concurrente de mÃºltiples hilos de bÃºsqueda.
*   **ğŸ§  AnÃ¡lisis SemÃ¡ntico con IA:** IntegraciÃ³n con OpenAI para analizar URLs y generar descripciones sintÃ©ticas y categorizaciÃ³n automÃ¡tica (Gobierno, Academia, Open Data).
*   **ğŸ¯ BÃºsqueda Dirigida:** Capacidad de subir listas de temÃ¡ticas personalizadas (CSV/TXT) para realizar bÃºsquedas especÃ­ficas usando *dorking* avanzado.
*   **ğŸ“¡ MonitorizaciÃ³n en Tiempo Real:** Dashboard interactivo con logs en vivo, barra de progreso y tabla de resultados dinÃ¡mica.
*   **ğŸ“¦ Todo en Uno:** Empaquetado en una sola imagen Docker (Multi-stage build) que sirve tanto la API como el Frontend estÃ¡tico.
*   **ğŸ’¾ ExportaciÃ³n de Datos:** Descarga inmediata de los hallazgos en formato CSV.

---

## ğŸ› ï¸ Stack TecnolÃ³gico

### Backend (Rust)
*   **Framework:** Axum
*   **Runtime:** Tokio
*   **HTTP Client:** Reqwest
*   **Embedding:** RustEmbed (para servir el frontend)
*   **Utilidades:** Scraper, Regex, Serde

### Frontend (React + Vite)
*   **Estilos:** TailwindCSS
*   **Iconos:** Lucide React
*   **ComunicaciÃ³n:** EventSource (SSE)

### Infraestructura
*   **ContainerizaciÃ³n:** Docker (Debian Slim)

---

## ğŸ“‹ Prerrequisitos

*   **Docker** instalado en tu sistema.
*   Una **API Key de OpenAI** (necesaria para la funciÃ³n de anÃ¡lisis inteligente, aunque el crawler funciona en modo bÃ¡sico sin ella).

---

## ğŸš€ InstalaciÃ³n y Despliegue

### OpciÃ³n 1: Docker (Recomendada)

1.  **Clonar el repositorio:**
    ```bash
    git clone https://github.com/tu-usuario/data-hunter-pro.git
    cd data-hunter-pro
    ```

2.  **Construir la imagen:**
    ```bash
    docker build -t data-hunter .
    ```

3.  **Ejecutar el contenedor:**
    Debes pasar tus variables de entorno, especialmente la `OPENAI_API_KEY`.
    ```bash
    docker run -d -p 3000:3000 \
      -e OPENAI_API_KEY="tu-api-key-aqui" \
      --name hunter-instance \
      data-hunter
    ```

4.  **Acceder:** Abre tu navegador en `http://localhost:3000`.

### OpciÃ³n 2: Desarrollo Local (Manual)

**Backend:**
```bash
# Necesitas Rust instalado
cargo run
```

**Frontend:**
```bash
# En otra terminal, dentro de la carpeta /web
cd web
npm install
npm run dev
```

---

## ğŸ“– GuÃ­a de Uso

1.  **ConfiguraciÃ³n del Objetivo:**
    *   En el panel izquierdo, establece el nÃºmero de **Fuentes Objetivo** (ej. 50 datasets).
2.  **BÃºsqueda TemÃ¡tica (Opcional):**
    *   Si deseas buscar algo especÃ­fico (ej. "Datos climÃ¡ticos", "Finanzas 2024"), crea un archivo `.txt` o `.csv` con una temÃ¡tica por lÃ­nea.
    *   Arrastra el archivo al Ã¡rea de **"TemÃ¡ticas CSV"**.
3.  **Iniciar Caza:**
    *   Presiona el botÃ³n **INICIAR**.
    *   VerÃ¡s los logs en tiempo real en la "AgÃ©ntic Console".
4.  **Resultados:**
    *   La tabla principal se llenarÃ¡ con los enlaces encontrados, la categorÃ­a detectada y la descripciÃ³n generada por la IA.
5.  **Exportar:**
    *   Haz clic en "EXPORTAR CSV" para descargar tus resultados.

---

## âš™ï¸ Variables de Entorno

El sistema utiliza un archivo `.env` o variables de entorno del sistema. Las principales son:

| Variable | DescripciÃ³n | Valor por defecto |
| :--- | :--- | :--- |
| `PORT` | Puerto de escucha del servidor | `3000` |
| `OPENAI_API_KEY` | Clave API para anÃ¡lisis inteligente | (Requerido para IA) |
| `AI_MODEL` | Modelo de OpenAI a utilizar | `gpt-4o-mini` |
| `RUST_LOG` | Nivel de log del backend | `info` |

---

## ğŸ“‚ Estructura del Proyecto

```
.
â”œâ”€â”€ Cargo.toml          # Dependencias de Rust
â”œâ”€â”€ Dockerfile          # ConstrucciÃ³n Multi-stage
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main.rs         # LÃ³gica del servidor, crawler y IA
â””â”€â”€ web/                # Frontend React
    â”œâ”€â”€ index.html
    â”œâ”€â”€ src/
    â”‚   â””â”€â”€ main.jsx    # UI LÃ³gica
    â”œâ”€â”€ package.json
    â””â”€â”€ vite.config.js
```

---

## ğŸ›¡ï¸ Aviso Legal

Esta herramienta estÃ¡ diseÃ±ada con fines educativos y de investigaciÃ³n para la localizaciÃ³n de datos abiertos (*Open Data*). El usuario es responsable de asegurar que el uso del crawler cumpla con los tÃ©rminos de servicio de los sitios web visitados y las regulaciones locales sobre scraping.

---

## ğŸ¤ ContribuciÃ³n

Â¡Las contribuciones son bienvenidas! Si tienes ideas para mejorar el algoritmo de bÃºsqueda o la interfaz:

1.  Haz un Fork del proyecto.
2.  Crea tu rama de caracterÃ­sticas (`git checkout -b feature/AmazingFeature`).
3.  Haz Commit de tus cambios (`git commit -m 'Add some AmazingFeature'`).
4.  Push a la rama (`git push origin feature/AmazingFeature`).
5.  Abre un Pull Request.

---

Hecho con â¤ï¸ y ğŸ¦€ (Rust).